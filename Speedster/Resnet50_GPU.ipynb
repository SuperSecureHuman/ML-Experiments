{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY3lmigwLZWZ"
      },
      "source": [
        "## GPU Optimized (Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0ZRCXCR9693",
        "outputId": "1876f9ef-f434-4347-cb23-41093c5bfd61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_VISIBLE_DEVICES=0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install speedster"
      ],
      "metadata": {
        "id": "zTbMnnMxLm_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m nebullvm.installers.auto_installer --compilers all"
      ],
      "metadata": {
        "id": "2WJEOWbTLoiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pillow==9.0.1"
      ],
      "metadata": {
        "id": "di6BYXYnM7hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skxEuemn171G"
      },
      "source": [
        "### Scenario 1 - No accuracy drop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVRLXrDi2VaG"
      },
      "source": [
        "First we load the model and optimize it using the Speedster API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2RbgGruAeQcf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d0dcef6-e1a6-41f9-a9c1-5c94b98da4c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-02-11 06:42:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU\u001b[0m\n",
            "\u001b[32m2023-02-11 06:42:14\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mNot enough data for splitting the DataManager. You should provide at least 100 data samples to allow a good split between train and test sets. Compression, calibration and precision checks will use the same data.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:42:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
            "\u001b[32m2023-02-11 06:42:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.013446285724639892 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:42:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:42:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.014631271362304688 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:42:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:42:33\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:42:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:42:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.005392551422119141 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:42:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:43:37\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:43:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:43:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.00635075569152832 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:43:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:43:43\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:43:43\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:44:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.003670930862426758 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:44:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:44:36\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
            "\n",
            "[Speedster results on Tesla T4]\n",
            "Metric       Original Model    Optimized Model    Improvement\n",
            "-----------  ----------------  -----------------  -------------\n",
            "backend      PYTORCH           TensorRT\n",
            "latency      0.0134 sec/batch  0.0037 sec/batch   3.66x\n",
            "throughput   74.37 data/sec    272.41 data/sec    3.66x\n",
            "model size   102.55 MB         113.66 MB          0%\n",
            "metric drop                    0\n",
            "techniques                     fp32\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from speedster import optimize_model, save_model, load_model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load a resnet as example\n",
        "model = models.resnet50().to(device)\n",
        "\n",
        "# Provide an input data for the model    \n",
        "input_data = [((torch.randn(1, 3, 256, 256), ), torch.tensor([0]))]\n",
        "\n",
        "# Run Speedster optimization\n",
        "optimized_model = optimize_model(\n",
        "  model, input_data=input_data, optimization_time=\"unconstrained\")\n",
        "\n",
        "# Try the optimized model\n",
        "x = torch.randn(1, 3, 256, 256).to(device)\n",
        "model.eval()\n",
        "res_optimized = optimized_model(x)\n",
        "res_original = model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMiuufyu2gD3"
      },
      "source": [
        "We can print the type of the optimized model to see which compiler was faster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jqbyebA7LZWc",
        "outputId": "917406b7-13b2-427c-cd7c-bf50c7ae4492",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PytorchONNXTensorRTInferenceLearner(network_parameters=ModelParams(batch_size=1, input_infos=[<nebullvm.tools.base.InputInfo object at 0x7fdbe6e5bdf0>], output_sizes=[(1000,)], dynamic_info=None), input_tfms=<nebullvm.tools.transformations.MultiStageTransformation object at 0x7fdbe48af9a0>, device=<Device.GPU: 'gpu'>)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "optimized_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBEtrYOd9699"
      },
      "source": [
        "Then, let's compare the performances:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GqxiCAbpfcwV"
      },
      "outputs": [],
      "source": [
        "from nebullvm.tools.benchmark import benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_0b0Bzwq-czD"
      },
      "outputs": [],
      "source": [
        "# Set the model to eval mode and move it to the available device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkt67_Orwlv4",
        "outputId": "4bb73c5e-d1b2-4d22-898b-3c1c0eeff43c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-02-11 06:44:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing warm up on 50 iterations: 100%|██████████| 50/50 [00:00<00:00, 103.99it/s]\n",
            "Performing benchmark on 1000 iterations: 100%|██████████| 1000/1000 [00:13<00:00, 72.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 1\n",
            "Average Throughput: 74.01 data/second\n",
            "Average Latency: 0.0135 seconds/data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "benchmark(model, input_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PodpaDVfwzT",
        "outputId": "8439ec4f-a033-4510-ac20-8f04fd610540"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-02-11 06:44:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing warm up on 50 iterations: 100%|██████████| 50/50 [00:00<00:00, 263.85it/s]\n",
            "Performing benchmark on 1000 iterations: 100%|██████████| 1000/1000 [00:03<00:00, 252.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 1\n",
            "Average Throughput: 255.90 data/second\n",
            "Average Latency: 0.0039 seconds/data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "benchmark(optimized_model, input_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBeRKNTI3iyK"
      },
      "source": [
        "## Scenario 2 - Accuracy drop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3wutIzfAMe_"
      },
      "source": [
        "In this scenario, we set a max threshold for the accuracy drop to 2%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fO1nGqpj3p7z",
        "outputId": "02376c67-2b21-45b9-bcc8-69ea62f91259",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-02-11 06:44:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU\u001b[0m\n",
            "\u001b[32m2023-02-11 06:45:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
            "\u001b[32m2023-02-11 06:45:05\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.01355921745300293 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:45:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:45:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.00795292854309082 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:45:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:45:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.014817476272583008 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:45:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:45:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.00420689582824707 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:45:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:46:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0016889572143554688 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:46:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PyTorchTensorRTCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:47:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0023703575134277344 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:47:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:47:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.006337881088256836 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:47:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:48:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0042819976806640625 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:48:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.DYNAMIC.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:48:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.25478053092956543 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:48:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:48:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.08857250213623047 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:48:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:48:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0041196346282958984 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:48:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:49:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0016710758209228516 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 06:49:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXTensorRTCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
            "\u001b[32m2023-02-11 06:50:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0013451576232910156 sec/iter\u001b[0m\n",
            "\n",
            "[Speedster results on Tesla T4]\n",
            "Metric       Original Model    Optimized Model    Improvement\n",
            "-----------  ----------------  -----------------  -------------\n",
            "backend      PYTORCH           TensorRT\n",
            "latency      0.0136 sec/batch  0.0013 sec/batch   10.08x\n",
            "throughput   73.75 data/sec    743.41 data/sec    10.08x\n",
            "model size   102.56 MB         27.91 MB           -72%\n",
            "metric drop                    0\n",
            "techniques                     int8\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from speedster import optimize_model\n",
        "\n",
        "# Load a resnet as example\n",
        "model = models.resnet50().to(device)\n",
        "\n",
        "# Provide 100 random input data for the model  \n",
        "input_data = [((torch.randn(1, 3, 256, 256), ), torch.tensor([0])) for _ in range(100)]\n",
        "\n",
        "# Run Speedster optimization\n",
        "optimized_model = optimize_model(\n",
        "  model, input_data=input_data, optimization_time=\"unconstrained\", metric=\"accuracy\", metric_drop_ths=0.02)\n",
        "# Try the optimized model\n",
        "x = torch.randn(1, 3, 256, 256).to(device)\n",
        "res = optimized_model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qFKHaHM6-GKm"
      },
      "outputs": [],
      "source": [
        "# Set the model to eval mode and move it to the available device\n",
        "model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MMrL3959hli",
        "outputId": "b203e3d2-8fcd-41ac-f5f2-c41aa3ef6b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-02-11 06:50:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing warm up on 50 iterations: 100%|██████████| 50/50 [00:00<00:00, 104.68it/s]\n",
            "Performing benchmark on 1000 iterations: 100%|██████████| 1000/1000 [00:13<00:00, 74.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 1\n",
            "Average Throughput: 76.19 data/second\n",
            "Average Latency: 0.0131 seconds/data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "benchmark(model, input_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IbAW0KA4Fm5",
        "outputId": "96a71fb9-514f-4250-c129-5a33ffd9f61f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32m2023-02-11 06:50:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on GPU\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Performing warm up on 50 iterations: 100%|██████████| 50/50 [00:00<00:00, 1117.78it/s]\n",
            "Performing benchmark on 1000 iterations: 100%|██████████| 1000/1000 [00:00<00:00, 1011.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 1\n",
            "Average Throughput: 1031.41 data/second\n",
            "Average Latency: 0.0010 seconds/data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "benchmark(optimized_model, input_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3CNckV55PngN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "nebullvm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "9c5b325fcd5468045cb00d6f6e8552712249001c4af40fff338bf9bc94161db4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}