{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CPU Optimized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0ZRCXCR9693",
        "outputId": "19096862-5c5c-4f9f-b2ad-3ce084ccf213"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=-1\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_VISIBLE_DEVICES=-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skxEuemn171G"
      },
      "source": [
        "### Scenario 1 - No accuracy drop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVRLXrDi2VaG"
      },
      "source": [
        "First we load the model and optimize it using the Speedster API:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2RbgGruAeQcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 11:28:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on CPU\u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:07\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mInstalled PyTorch does not have cuda support. Please ensure that torch.cuda.is_available() returns True by installing the proper version of PyTorch. \u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:07\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mNot enough data for splitting the DataManager. You should provide at least 100 data samples to allow a good split between train and test sets. Compression, calibration and precision checks will use the same data.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.044352409839630125 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:14\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mInstalled PyTorch does not have cuda support. Please ensure that torch.cuda.is_available() returns True by installing the proper version of PyTorch. \u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.03699028491973877 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with DeepSparseCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.015450000762939453 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.023233652114868164 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:22\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: None.\u001b[0m\n",
            "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
            "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
            "[ SUCCESS ] Generated IR version 11 model.\n",
            "[ SUCCESS ] XML file: /tmp/tmp__v474zc/fp32/temp.xml\n",
            "[ SUCCESS ] BIN file: /tmp/tmp__v474zc/fp32/temp.bin\n",
            "\u001b[32m2023-02-11 11:28:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.01634812355041504 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:28:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: QuantizationType.HALF.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARNING ]  Use of deprecated cli option --data_type detected. Option use in the following releases will be fatal. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
            "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
            "[ SUCCESS ] Generated IR version 11 model.\n",
            "[ SUCCESS ] XML file: /tmp/tmp__v474zc/fp32/temp.xml\n",
            "[ SUCCESS ] BIN file: /tmp/tmp__v474zc/fp32/temp.bin\n",
            "\u001b[32m2023-02-11 11:28:27\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
            "\n",
            "[Speedster results on 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz]\n",
            "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
            "┃ Metric      ┃ Original Model   ┃ Optimized Model   ┃ Improvement   ┃\n",
            "┣━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━┫\n",
            "┃ backend     ┃ PYTORCH          ┃ DeepSparse        ┃               ┃\n",
            "┃ latency     ┃ 0.0444 sec/batch ┃ 0.0155 sec/batch  ┃ 2.87x         ┃\n",
            "┃ throughput  ┃ 22.55 data/sec   ┃ 64.72 data/sec    ┃ 2.87x         ┃\n",
            "┃ model size  ┃ 102.55 MB        ┃ 102.06 MB         ┃ 0%            ┃\n",
            "┃ metric drop ┃                  ┃ 0                 ┃               ┃\n",
            "┃ techniques  ┃                  ┃ fp32              ┃               ┃\n",
            "┗━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━┛\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from speedster import optimize_model, save_model, load_model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load a resnet as example\n",
        "model = models.resnet50().to(device)\n",
        "\n",
        "# Provide an input data for the model    \n",
        "input_data = [((torch.randn(1, 3, 256, 256), ), torch.tensor([0]))]\n",
        "\n",
        "# Run Speedster optimization\n",
        "optimized_model = optimize_model(\n",
        "  model, input_data=input_data, optimization_time=\"unconstrained\",\n",
        "device=\"cpu\")\n",
        "\n",
        "# Try the optimized model\n",
        "x = torch.randn(1, 3, 256, 256).to(device)\n",
        "model.eval()\n",
        "res_optimized = optimized_model(x)\n",
        "res_original = model(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMiuufyu2gD3"
      },
      "source": [
        "We can print the type of the optimized model to see which compiler was faster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PytorchDeepSparseInferenceLearner(network_parameters=ModelParams(batch_size=1, input_infos=[<nebullvm.tools.base.InputInfo object at 0x7f2202bc3a60>], output_sizes=[(1000,)], dynamic_info=None), input_tfms=None, device=<Device.CPU: 'cpu'>)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimized_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBEtrYOd9699"
      },
      "source": [
        "Then, let's compare the performances:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GqxiCAbpfcwV"
      },
      "outputs": [],
      "source": [
        "from nebullvm.tools.benchmark import benchmark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_0b0Bzwq-czD"
      },
      "outputs": [],
      "source": [
        "# Set the model to eval mode and move it to the available device\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkt67_Orwlv4",
        "outputId": "fc10c03c-c3ad-44d4-9fd6-c9b6dc0256c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 11:30:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Performing warm up on 50 iterations: 100%|██████████| 50/50 [00:02<00:00, 21.38it/s]\n",
            "Performing benchmark on 1000 iterations: 100%|██████████| 1000/1000 [00:48<00:00, 20.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 1\n",
            "Average Throughput: 20.62 data/second\n",
            "Average Latency: 0.0485 seconds/data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "benchmark(model, input_data, device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PodpaDVfwzT",
        "outputId": "27a42560-93a2-4c19-e68d-360093fe914c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 11:31:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Performing warm up on 50 iterations: 100%|██████████| 50/50 [00:01<00:00, 45.70it/s]\n",
            "Performing benchmark on 1000 iterations: 100%|██████████| 1000/1000 [00:20<00:00, 49.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 1\n",
            "Average Throughput: 49.62 data/second\n",
            "Average Latency: 0.0202 seconds/data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "benchmark(optimized_model, input_data, device='cpu')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBeRKNTI3iyK"
      },
      "source": [
        "## Scenario 2 - Accuracy drop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3wutIzfAMe_"
      },
      "source": [
        "In this scenario, we set a max threshold for the accuracy drop to 2%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fO1nGqpj3p7z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 11:46:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on CPU\u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:11\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mInstalled PyTorch does not have cuda support. Please ensure that torch.cuda.is_available() returns True by installing the proper version of PyTorch. \u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.040916502475738525 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:24\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mInstalled PyTorch does not have cuda support. Please ensure that torch.cuda.is_available() returns True by installing the proper version of PyTorch. \u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.036638498306274414 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.DYNAMIC.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.03601813316345215 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.011111736297607422 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with DeepSparseCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.01997232437133789 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with IntelNeuralCompressorCompiler and q_type: QuantizationType.DYNAMIC.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-11 11:46:54 [WARNING] Force convert framework model to neural_compressor model.\n",
            "2023-02-11 11:46:54 [INFO] Because both eval_dataloader_cfg and user-defined eval_func are None, automatically setting 'tuning.exit_policy.performance_only = True'.\n",
            "2023-02-11 11:46:54 [INFO] Generate a fake evaluation function.\n",
            "2023-02-11 11:46:54 [INFO] Pass query framework capability elapsed time: 137.24 ms\n",
            "2023-02-11 11:46:54 [INFO] Get FP32 model baseline.\n",
            "2023-02-11 11:46:54 [INFO] Save tuning history to /home/venom/repo/nebullvm/notebooks/speedster/pytorch/nc_workspace/2023-02-11_11-26-03/./history.snapshot.\n",
            "2023-02-11 11:46:54 [INFO] FP32 baseline is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n",
            "2023-02-11 11:46:55 [INFO] |******Mixed Precision Statistics******|\n",
            "2023-02-11 11:46:55 [INFO] +---------------+-----------+----------+\n",
            "2023-02-11 11:46:55 [INFO] |    Op Type    |   Total   |   INT8   |\n",
            "2023-02-11 11:46:55 [INFO] +---------------+-----------+----------+\n",
            "2023-02-11 11:46:55 [INFO] |     Linear    |     1     |    1     |\n",
            "2023-02-11 11:46:55 [INFO] +---------------+-----------+----------+\n",
            "2023-02-11 11:46:55 [INFO] Pass quantize model elapsed time: 218.63 ms\n",
            "2023-02-11 11:46:55 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 1.0000|1.0000, Duration (seconds) (int8|fp32): 0.0000|0.0000], Best tune result is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n",
            "2023-02-11 11:46:55 [INFO] |**********************Tune Result Statistics**********************|\n",
            "2023-02-11 11:46:55 [INFO] +--------------------+----------+---------------+------------------+\n",
            "2023-02-11 11:46:55 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |\n",
            "2023-02-11 11:46:55 [INFO] +--------------------+----------+---------------+------------------+\n",
            "2023-02-11 11:46:55 [INFO] |      Accuracy      | 1.0000   |    1.0000     |     1.0000       |\n",
            "2023-02-11 11:46:55 [INFO] | Duration (seconds) | 0.0000   |    0.0000     |     0.0000       |\n",
            "2023-02-11 11:46:55 [INFO] +--------------------+----------+---------------+------------------+\n",
            "2023-02-11 11:46:55 [INFO] Save tuning history to /home/venom/repo/nebullvm/notebooks/speedster/pytorch/nc_workspace/2023-02-11_11-26-03/./history.snapshot.\n",
            "2023-02-11 11:46:55 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
            "2023-02-11 11:46:55 [INFO] Save deploy yaml to /home/venom/repo/nebullvm/notebooks/speedster/pytorch/nc_workspace/2023-02-11_11-26-03/deploy.yaml\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 11:46:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.04393506050109863 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:46:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with IntelNeuralCompressorCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-11 11:46:57 [WARNING] Force convert framework model to neural_compressor model.\n",
            "2023-02-11 11:46:57 [INFO] Pass query framework capability elapsed time: 134.41 ms\n",
            "2023-02-11 11:46:57 [INFO] Get FP32 model baseline.\n",
            "2023-02-11 11:47:00 [INFO] Save tuning history to /home/venom/repo/nebullvm/notebooks/speedster/pytorch/nc_workspace/2023-02-11_11-26-03/./history.snapshot.\n",
            "2023-02-11 11:47:00 [INFO] FP32 baseline is: [Accuracy: 0.0000, Duration (seconds): 3.5599]\n",
            "2023-02-11 11:47:12 [INFO] |******Mixed Precision Statistics******|\n",
            "2023-02-11 11:47:12 [INFO] +----------------------+-------+-------+\n",
            "2023-02-11 11:47:12 [INFO] |       Op Type        | Total |  INT8 |\n",
            "2023-02-11 11:47:12 [INFO] +----------------------+-------+-------+\n",
            "2023-02-11 11:47:12 [INFO] | quantize_per_tensor  |   1   |   1   |\n",
            "2023-02-11 11:47:12 [INFO] |      ConvReLU2d      |   33  |   33  |\n",
            "2023-02-11 11:47:12 [INFO] |      MaxPool2d       |   1   |   1   |\n",
            "2023-02-11 11:47:12 [INFO] |        Conv2d        |   20  |   20  |\n",
            "2023-02-11 11:47:12 [INFO] |       add_relu       |   16  |   16  |\n",
            "2023-02-11 11:47:12 [INFO] |  AdaptiveAvgPool2d   |   1   |   1   |\n",
            "2023-02-11 11:47:12 [INFO] |       flatten        |   1   |   1   |\n",
            "2023-02-11 11:47:12 [INFO] |        Linear        |   1   |   1   |\n",
            "2023-02-11 11:47:12 [INFO] |      dequantize      |   1   |   1   |\n",
            "2023-02-11 11:47:12 [INFO] +----------------------+-------+-------+\n",
            "2023-02-11 11:47:12 [INFO] Pass quantize model elapsed time: 11827.85 ms\n",
            "2023-02-11 11:47:13 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 0.0000|0.0000, Duration (seconds) (int8|fp32): 1.0354|3.5599], Best tune result is: [Accuracy: 0.0000, Duration (seconds): 1.0354]\n",
            "2023-02-11 11:47:13 [INFO] |**********************Tune Result Statistics**********************|\n",
            "2023-02-11 11:47:13 [INFO] +--------------------+----------+---------------+------------------+\n",
            "2023-02-11 11:47:13 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |\n",
            "2023-02-11 11:47:13 [INFO] +--------------------+----------+---------------+------------------+\n",
            "2023-02-11 11:47:13 [INFO] |      Accuracy      | 0.0000   |    0.0000     |     0.0000       |\n",
            "2023-02-11 11:47:13 [INFO] | Duration (seconds) | 3.5599   |    1.0354     |     1.0354       |\n",
            "2023-02-11 11:47:13 [INFO] +--------------------+----------+---------------+------------------+\n",
            "2023-02-11 11:47:13 [INFO] Save tuning history to /home/venom/repo/nebullvm/notebooks/speedster/pytorch/nc_workspace/2023-02-11_11-26-03/./history.snapshot.\n",
            "2023-02-11 11:47:13 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
            "2023-02-11 11:47:13 [INFO] Save deploy yaml to /home/venom/repo/nebullvm/notebooks/speedster/pytorch/nc_workspace/2023-02-11_11-26-03/deploy.yaml\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 11:47:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.013910055160522461 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:47:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:47:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.03476357460021973 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:47:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:47:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.08549785614013672 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:47:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.DYNAMIC.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:47:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.031424760818481445 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:47:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
            "\u001b[32m2023-02-11 11:47:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.012981414794921875 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:47:38\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: None.\u001b[0m\n",
            "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
            "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
            "[ SUCCESS ] Generated IR version 11 model.\n",
            "[ SUCCESS ] XML file: /tmp/tmpygrxxiw9/fp32/temp.xml\n",
            "[ SUCCESS ] BIN file: /tmp/tmpygrxxiw9/fp32/temp.bin\n",
            "\u001b[32m2023-02-11 11:47:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.021528244018554688 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:47:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: QuantizationType.HALF.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARNING ]  Use of deprecated cli option --data_type detected. Option use in the following releases will be fatal. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
            "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
            "[ SUCCESS ] Generated IR version 11 model.\n",
            "[ SUCCESS ] XML file: /tmp/tmpygrxxiw9/fp32/temp.xml\n",
            "[ SUCCESS ] BIN file: /tmp/tmpygrxxiw9/fp32/temp.bin\n",
            "\u001b[32m2023-02-11 11:47:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.02113056182861328 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 11:47:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
            "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
            "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
            "[ SUCCESS ] Generated IR version 11 model.\n",
            "[ SUCCESS ] XML file: /tmp/tmpygrxxiw9/fp32/temp.xml\n",
            "[ SUCCESS ] BIN file: /tmp/tmpygrxxiw9/fp32/temp.bin\n",
            "\u001b[32m2023-02-11 11:48:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.005305767059326172 sec/iter\u001b[0m\n",
            "\n",
            "[Speedster results on 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz]\n",
            "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
            "┃ Metric      ┃ Original Model   ┃ Optimized Model   ┃ Improvement   ┃\n",
            "┣━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━┫\n",
            "┃ backend     ┃ PYTORCH          ┃ OpenVINO          ┃               ┃\n",
            "┃ latency     ┃ 0.0409 sec/batch ┃ 0.0053 sec/batch  ┃ 7.71x         ┃\n",
            "┃ throughput  ┃ 24.44 data/sec   ┃ 188.47 data/sec   ┃ 7.71x         ┃\n",
            "┃ model size  ┃ 102.55 MB        ┃ 25.98 MB          ┃ -74%          ┃\n",
            "┃ metric drop ┃                  ┃ 0                 ┃               ┃\n",
            "┃ techniques  ┃                  ┃ int8              ┃               ┃\n",
            "┗━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━┛\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.models as models\n",
        "from speedster import optimize_model\n",
        "\n",
        "# Load a resnet as example\n",
        "model = models.resnet50().to(device)\n",
        "\n",
        "# Provide 100 random input data for the model  \n",
        "input_data = [((torch.randn(1, 3, 256, 256), ), torch.tensor([0])) for _ in range(100)]\n",
        "\n",
        "# Run Speedster optimization\n",
        "optimized_model = optimize_model(\n",
        "  model, input_data=input_data, optimization_time=\"unconstrained\", metric=\"accuracy\", metric_drop_ths=0.02\n",
        ", device=\"cpu\")\n",
        "\n",
        "# Try the optimized model\n",
        "x = torch.randn(1, 3, 256, 256).to(device)\n",
        "res = optimized_model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "qFKHaHM6-GKm"
      },
      "outputs": [],
      "source": [
        "# Set the model to eval mode and move it to the available device\n",
        "model.eval()\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MMrL3959hli",
        "outputId": "2e8d27ec-a9f3-4f70-8c75-a0df974f2653"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 11:48:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Performing warm up on 50 iterations: 100%|██████████| 50/50 [00:02<00:00, 18.63it/s]\n",
            "Performing benchmark on 1000 iterations: 100%|██████████| 1000/1000 [00:42<00:00, 23.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 1\n",
            "Average Throughput: 23.53 data/second\n",
            "Average Latency: 0.0425 seconds/data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "benchmark(model, input_data, device='cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IbAW0KA4Fm5",
        "outputId": "48d83c89-5687-42aa-a3b8-6989bcb66aa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 11:49:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning benchmark on CPU\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Performing warm up on 50 iterations: 100%|██████████| 50/50 [00:00<00:00, 145.15it/s]\n",
            "Performing benchmark on 1000 iterations: 100%|██████████| 1000/1000 [00:07<00:00, 142.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size: 1\n",
            "Average Throughput: 144.27 data/second\n",
            "Average Latency: 0.0069 seconds/data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "benchmark(optimized_model, input_data, device='cpu')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "nebullvm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "9c5b325fcd5468045cb00d6f6e8552712249001c4af40fff338bf9bc94161db4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
