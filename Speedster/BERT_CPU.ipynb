{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "7Ex-UkOgUHlV",
      "metadata": {
        "id": "7Ex-UkOgUHlV"
      },
      "source": [
        "## BERT CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d527d63b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d527d63b",
        "outputId": "9fdc3e35-8953-4f71-8578-421f5a397fd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=-1\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_VISIBLE_DEVICES=-1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73072506",
      "metadata": {
        "id": "73072506"
      },
      "source": [
        "## Model and Dataset setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d55115",
      "metadata": {
        "id": "e4d55115"
      },
      "source": [
        "We chose BERT as the pre-trained model that we want to optimize. Let's download both the pre-trained model and the tokenizer from the Hugging Face model hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d633cf21",
      "metadata": {
        "id": "d633cf21",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/venom/miniconda3/envs/nebullvm/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2023-02-11 12:51:29.523648: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-02-11 12:51:29.840977: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-02-11 12:51:30.884280: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/venom/lib/:/usr/local/cuda/lib64::/usr/local/tensorrt/lib/\n",
            "2023-02-11 12:51:30.884401: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/venom/lib/:/usr/local/cuda/lib64::/usr/local/tensorrt/lib/\n",
            "2023-02-11 12:51:30.884406: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased', torchscript=True)\n",
        "\n",
        "# Move the model to gpu if available and set eval mode\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11aa0739",
      "metadata": {
        "id": "11aa0739"
      },
      "source": [
        "Let's create an example dataset with some random sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "cbbfeeb2",
      "metadata": {
        "id": "cbbfeeb2"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "sentences = [\n",
        "    \"Mars is the fourth planet from the Sun.\",\n",
        "    \"has a crust primarily composed of elements\",\n",
        "    \"However, it is unknown\",\n",
        "    \"can be viewed from Earth\",\n",
        "    \"It was the Romans\",\n",
        "]\n",
        "\n",
        "len_dataset = 100\n",
        "\n",
        "texts = []\n",
        "for _ in range(len_dataset):\n",
        "    n_times = random.randint(1, 30)\n",
        "    texts.append(\" \".join(random.choice(sentences) for _ in range(n_times)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a09f9424",
      "metadata": {
        "id": "a09f9424"
      },
      "outputs": [],
      "source": [
        "encoded_inputs = [tokenizer(text, return_tensors=\"pt\") for text in texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17040431",
      "metadata": {
        "id": "17040431"
      },
      "source": [
        "## Speed up inference with Speedster: no metric drop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44ddc21d",
      "metadata": {
        "id": "44ddc21d"
      },
      "source": [
        "It's now time of improving a bit the performance in terms of speed. Let's use `Speedster`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f9d934f6",
      "metadata": {
        "id": "f9d934f6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-11 12:51:51.543620: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:267] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2023-02-11 12:51:51.543699: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: predator\n",
            "2023-02-11 12:51:51.543704: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: predator\n",
            "2023-02-11 12:51:51.543796: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 525.89.2\n",
            "2023-02-11 12:51:51.543814: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 525.89.2\n",
            "2023-02-11 12:51:51.543817: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 525.89.2\n",
            "WARNING: [Torch-TensorRT] - Unable to read CUDA capable devices. Return status: 100\n",
            "ERROR: [Torch-TensorRT] - Cannot get current device\n",
            "ERROR: [Torch-TensorRT] - Cannot get current device\n",
            "ERROR: [Torch-TensorRT] - Cannot get current device\n",
            "ERROR: [Torch-TensorRT] - Cannot get current device\n",
            "ERROR: [Torch-TensorRT] - Cannot get current device\n"
          ]
        }
      ],
      "source": [
        "from speedster import optimize_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76248033",
      "metadata": {
        "id": "76248033"
      },
      "source": [
        "Using Speedster is very simple and straightforward! Just use the `optimize_model` function and provide as input the model, some input data as example and the optimization time mode. Optionally a dynamic_info dictionary can be also provided, in order to support inputs with dynamic shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "zPC_EDwEJIM0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPC_EDwEJIM0",
        "outputId": "297c448a-6662-4f57-dc3f-5d7fdcfefd26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 12:45:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on CPU\u001b[0m\n",
            "\u001b[32m2023-02-11 12:45:41\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mInstalled PyTorch does not have cuda support. Please ensure that torch.cuda.is_available() returns True by installing the proper version of PyTorch. \u001b[0m\n",
            "\u001b[32m2023-02-11 12:45:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
            "\u001b[32m2023-02-11 12:45:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.06308063268661498 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 12:46:01\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mInstalled PyTorch does not have cuda support. Please ensure that torch.cuda.is_available() returns True by installing the proper version of PyTorch. \u001b[0m\n",
            "\u001b[32m2023-02-11 12:46:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:46:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.06523668766021729 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 12:46:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with DeepSparseCompiler and q_type: None.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeepSparse, Copyright 2021-present / Neuralmagic, Inc. version: 1.3.2 COMMUNITY | (7d31c4bf) (release) (optimized) (system=avx512_vnni, binary=avx512)\n",
            "[nm_ort 7f9608d1b500 >WARN<  is_supported_graph /home/ubuntu/build/nyann/src/onnxruntime_neuralmagic/supported/ops.cc:203] Warning: Optimized runtime disabled - Detected dynamic input input_0 dim 1. Set inputs to static shapes to enable optimal performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 12:46:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.06429529190063477 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 12:46:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:46:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.04046010971069336 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 12:46:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:46:30\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:46:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: None.\u001b[0m\n",
            "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
            "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
            "[ SUCCESS ] Generated IR version 11 model.\n",
            "[ SUCCESS ] XML file: /tmp/tmp7ss5xzcm/fp32/temp.xml\n",
            "[ SUCCESS ] BIN file: /tmp/tmp7ss5xzcm/fp32/temp.bin\n",
            "\u001b[32m2023-02-11 12:46:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.047566890716552734 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 12:46:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: QuantizationType.HALF.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARNING ]  Use of deprecated cli option --data_type detected. Option use in the following releases will be fatal. \n",
            "Warning: One or more of the values of the Constant can't fit in the float16 data type. Those values were casted to the nearest limit value, the model can produce incorrect results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
            "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
            "[ SUCCESS ] Generated IR version 11 model.\n",
            "[ SUCCESS ] XML file: /tmp/tmp7ss5xzcm/fp32/temp.xml\n",
            "[ SUCCESS ] BIN file: /tmp/tmp7ss5xzcm/fp32/temp.bin\n",
            "\u001b[32m2023-02-11 12:46:43\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
            "\n",
            "[Speedster results on 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz]\n",
            "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
            "┃ Metric      ┃ Original Model   ┃ Optimized Model   ┃ Improvement   ┃\n",
            "┣━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━┫\n",
            "┃ backend     ┃ PYTORCH          ┃ ONNXRuntime       ┃               ┃\n",
            "┃ latency     ┃ 0.0631 sec/batch ┃ 0.0405 sec/batch  ┃ 1.56x         ┃\n",
            "┃ throughput  ┃ 15.85 data/sec   ┃ 24.72 data/sec    ┃ 1.56x         ┃\n",
            "┃ model size  ┃ 438.02 MB        ┃ 438.23 MB         ┃ 0%            ┃\n",
            "┃ metric drop ┃                  ┃ 0                 ┃               ┃\n",
            "┃ techniques  ┃                  ┃ fp32              ┃               ┃\n",
            "┗━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━┛\n",
            "\n",
            "Max speed-up with your input parameters is 1.56x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dynamic_info = {\n",
        "    \"inputs\": [\n",
        "        {0: 'batch', 1: 'num_tokens'},\n",
        "        {0: 'batch', 1: 'num_tokens'},\n",
        "        {0: 'batch', 1: 'num_tokens'},\n",
        "    ],\n",
        "    \"outputs\": [\n",
        "        {0: 'batch', 1: 'num_tokens'},\n",
        "        {0: 'batch'},\n",
        "    ]\n",
        "}\n",
        "\n",
        "optimized_model = optimize_model(\n",
        "    model=model,\n",
        "    input_data=encoded_inputs,\n",
        "    optimization_time=\"constrained\",\n",
        "    ignore_compilers=[\"tensor_rt\", \"tvm\"],  # TensorRT does not work for this model\n",
        "    dynamic_info=dynamic_info,\n",
        "    device='cpu',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "98c6ab09",
      "metadata": {
        "id": "98c6ab09"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Move inputs to gpu if available\n",
        "encoded_inputs = [tokenizer(text, return_tensors=\"pt\").to(device) for text in texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e5b3b21",
      "metadata": {
        "id": "6e5b3b21"
      },
      "source": [
        "Let's run the prediction 100 times to calculate the average response time of the original model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d3bc5c98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3bc5c98",
        "outputId": "deeafd40-b903-43c6-def0-97283e1b5720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average response time for original DistilBERT: 60.156662464141846 ms\n"
          ]
        }
      ],
      "source": [
        "times = []\n",
        "\n",
        "# Warmup for 30 iterations\n",
        "for encoded_input in encoded_inputs[:30]:\n",
        "    with torch.no_grad():\n",
        "        final_out = model(**encoded_input)\n",
        "\n",
        "# Benchmark\n",
        "for encoded_input in encoded_inputs:\n",
        "    st = time.time()\n",
        "    with torch.no_grad():\n",
        "        final_out = model(**encoded_input)\n",
        "    times.append(time.time()-st)\n",
        "original_model_time = sum(times)/len(times)*1000\n",
        "print(f\"Average response time for original DistilBERT: {original_model_time} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3db0a7a1",
      "metadata": {
        "id": "3db0a7a1"
      },
      "source": [
        "Let's run the prediction 100 times to calculate the average response time of the optimized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "a3e83997",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3e83997",
        "outputId": "47765b66-b19d-4382-c25f-5532827f9c77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average response time for optimized BERT (no metric drop): 57.145066261291504 ms\n"
          ]
        }
      ],
      "source": [
        "times = []\n",
        "\n",
        "# Warmup for 30 iterations\n",
        "for encoded_input in encoded_inputs[:30]:\n",
        "    with torch.no_grad():\n",
        "        final_out = optimized_model(**encoded_input)\n",
        "\n",
        "# Benchmark\n",
        "for encoded_input in encoded_inputs:\n",
        "    st = time.time()\n",
        "    with torch.no_grad():\n",
        "        final_out = optimized_model(**encoded_input)\n",
        "    times.append(time.time()-st)\n",
        "optimized_model_time = sum(times)/len(times)*1000\n",
        "print(f\"Average response time for optimized BERT (no metric drop): {optimized_model_time} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceb60d8c",
      "metadata": {
        "id": "ceb60d8c"
      },
      "source": [
        "## Speed up inference with Speedster: metric drop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1950d5",
      "metadata": {
        "id": "7b1950d5"
      },
      "source": [
        "This time we will use the `metric_drop_ths` argument to accept a little drop in terms of precision, in order to enable quantization and obtain an higher speedup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "de5721d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de5721d8",
        "outputId": "17fc2c0f-9496-466b-b0fa-348fc8846507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 12:51:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on CPU\u001b[0m\n",
            "\u001b[32m2023-02-11 12:51:57\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mInstalled PyTorch does not have cuda support. Please ensure that torch.cuda.is_available() returns True by installing the proper version of PyTorch. \u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.05229429244995117 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:15\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mInstalled PyTorch does not have cuda support. Please ensure that torch.cuda.is_available() returns True by installing the proper version of PyTorch. \u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.04668235778808594 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.DYNAMIC.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:21\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:21\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mOptimization failed with DeepLearningFramework.PYTORCH interface of ModelCompiler.TORCHSCRIPT. Got error Embedding quantization is only supported with float_qparams_weight_only_qconfig.. If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:21\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:21\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mOptimization failed with DeepLearningFramework.PYTORCH interface of ModelCompiler.TORCHSCRIPT. Got error torch.histogram: input tensor and hist tensor should have the same dtype, but got input long int and hist float. If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with DeepSparseCompiler and q_type: None.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DeepSparse, Copyright 2021-present / Neuralmagic, Inc. version: 1.3.2 COMMUNITY | (7d31c4bf) (release) (optimized) (system=avx512_vnni, binary=avx512)\n",
            "[nm_ort 7f97a434e500 >WARN<  is_supported_graph /home/ubuntu/build/nyann/src/onnxruntime_neuralmagic/supported/ops.cc:203] Warning: Optimized runtime disabled - Detected dynamic input input_0 dim 1. Set inputs to static shapes to enable optimal performance.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 12:52:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.06431055068969727 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with IntelNeuralCompressorCompiler and q_type: QuantizationType.DYNAMIC.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-11 12:52:27 [WARNING] Force convert framework model to neural_compressor model.\n",
            "2023-02-11 12:52:27 [INFO] Because both eval_dataloader_cfg and user-defined eval_func are None, automatically setting 'tuning.exit_policy.performance_only = True'.\n",
            "2023-02-11 12:52:27 [INFO] Generate a fake evaluation function.\n",
            "2023-02-11 12:52:28 [INFO] Pass query framework capability elapsed time: 356.84 ms\n",
            "2023-02-11 12:52:28 [INFO] Get FP32 model baseline.\n",
            "2023-02-11 12:52:28 [INFO] Save tuning history to /home/venom/repo/nebullvm/notebooks/speedster/pytorch/nc_workspace/2023-02-11_12-51-54/./history.snapshot.\n",
            "2023-02-11 12:52:28 [INFO] FP32 baseline is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n",
            "2023-02-11 12:52:28 [INFO] Fx trace of the entire model failed, We will conduct auto quantization\n",
            "2023-02-11 12:52:30 [INFO] |******Mixed Precision Statistics******|\n",
            "2023-02-11 12:52:30 [INFO] +-----------------+----------+---------+\n",
            "2023-02-11 12:52:30 [INFO] |     Op Type     |  Total   |   INT8  |\n",
            "2023-02-11 12:52:30 [INFO] +-----------------+----------+---------+\n",
            "2023-02-11 12:52:30 [INFO] |    Embedding    |    3     |    3    |\n",
            "2023-02-11 12:52:30 [INFO] |      Linear     |    73    |    73   |\n",
            "2023-02-11 12:52:30 [INFO] +-----------------+----------+---------+\n",
            "2023-02-11 12:52:30 [INFO] Pass quantize model elapsed time: 2341.11 ms\n",
            "2023-02-11 12:52:30 [INFO] Tune 1 result is: [Accuracy (int8|fp32): 1.0000|1.0000, Duration (seconds) (int8|fp32): 0.0000|0.0000], Best tune result is: [Accuracy: 1.0000, Duration (seconds): 0.0000]\n",
            "2023-02-11 12:52:30 [INFO] |**********************Tune Result Statistics**********************|\n",
            "2023-02-11 12:52:30 [INFO] +--------------------+----------+---------------+------------------+\n",
            "2023-02-11 12:52:30 [INFO] |     Info Type      | Baseline | Tune 1 result | Best tune result |\n",
            "2023-02-11 12:52:30 [INFO] +--------------------+----------+---------------+------------------+\n",
            "2023-02-11 12:52:30 [INFO] |      Accuracy      | 1.0000   |    1.0000     |     1.0000       |\n",
            "2023-02-11 12:52:30 [INFO] | Duration (seconds) | 0.0000   |    0.0000     |     0.0000       |\n",
            "2023-02-11 12:52:30 [INFO] +--------------------+----------+---------------+------------------+\n",
            "2023-02-11 12:52:30 [INFO] Save tuning history to /home/venom/repo/nebullvm/notebooks/speedster/pytorch/nc_workspace/2023-02-11_12-51-54/./history.snapshot.\n",
            "2023-02-11 12:52:30 [INFO] Specified timeout or max trials is reached! Found a quantized model which meet accuracy goal. Exit.\n",
            "2023-02-11 12:52:30 [INFO] Save deploy yaml to /home/venom/repo/nebullvm/notebooks/speedster/pytorch/nc_workspace/2023-02-11_12-51-54/deploy.yaml\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 12:52:31\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with IntelNeuralCompressorCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-11 12:52:31 [WARNING] Force convert framework model to neural_compressor model.\n",
            "2023-02-11 12:52:31 [INFO] Pass query framework capability elapsed time: 360.04 ms\n",
            "2023-02-11 12:52:31 [INFO] Get FP32 model baseline.\n",
            "2023-02-11 12:52:31 [WARNING] The dataloader didn't include label, will try input without label!\n",
            "2023-02-11 12:52:31 [ERROR] Unexpected exception AssertionError('The dataloader must include label to measure the metric!') happened during tuning.\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/venom/.local/lib/python3.8/site-packages/neural_compressor/adaptor/pytorch.py\", line 885, in eval_func\n",
            "    metric.update(output, label)\n",
            "  File \"/home/venom/.local/lib/python3.8/site-packages/neural_compressor/experimental/metric/metric.py\", line 969, in update\n",
            "    preds, labels = _topk_shape_validate(preds, labels)\n",
            "  File \"/home/venom/.local/lib/python3.8/site-packages/neural_compressor/experimental/metric/metric.py\", line 426, in _topk_shape_validate\n",
            "    if len(preds.shape) == 1:\n",
            "AttributeError: 'tuple' object has no attribute 'shape'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/venom/.local/lib/python3.8/site-packages/neural_compressor/experimental/quantization.py\", line 177, in execute\n",
            "    self.strategy.traverse()\n",
            "  File \"/home/venom/.local/lib/python3.8/site-packages/neural_compressor/strategy/strategy.py\", line 206, in traverse\n",
            "    self.baseline = self._evaluate(self.model)\n",
            "  File \"/home/venom/.local/lib/python3.8/site-packages/neural_compressor/strategy/strategy.py\", line 822, in _evaluate\n",
            "    val = self.objectives.evaluate(eval_func, model)\n",
            "  File \"/home/venom/.local/lib/python3.8/site-packages/neural_compressor/objective.py\", line 342, in evaluate\n",
            "    acc = eval_func(model)\n",
            "  File \"/home/venom/.local/lib/python3.8/site-packages/neural_compressor/utils/create_obj_from_config.py\", line 154, in eval_func\n",
            "    return adaptor.evaluate(model, dataloader, postprocess,\n",
            "  File \"/home/venom/.local/lib/python3.8/site-packages/neural_compressor/adaptor/pytorch.py\", line 2933, in evaluate\n",
            "    return self.model_eval(model_, dataloader, postprocess, metrics, measurer, iteration)\n",
            "  File \"/home/venom/.local/lib/python3.8/site-packages/neural_compressor/adaptor/pytorch.py\", line 976, in model_eval\n",
            "    results = self.eval_func(model, dataloader, postprocess, metrics, measurer,\n",
            "  File \"/home/venom/.local/lib/python3.8/site-packages/neural_compressor/adaptor/pytorch.py\", line 909, in eval_func\n",
            "    assert False, \"The dataloader must include label to measure the metric!\"\n",
            "AssertionError: The dataloader must include label to measure the metric!\n",
            "2023-02-11 12:52:31 [ERROR] Specified timeout or max trials is reached! Not found any quantized model which meet accuracy goal. Exit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 12:52:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0500410795211792 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.19515776634216309 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 12:52:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.DYNAMIC.\u001b[0m\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.0/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.0/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.1/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.1/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.2/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.2/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.3/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.3/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.4/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.4/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.5/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.5/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.6/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.6/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.7/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.7/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.8/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.8/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.9/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.9/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.10/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.10/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.11/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.11/attention/self/MatMul_1]\n",
            "\u001b[32m2023-02-11 12:53:04\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:53:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:53:22\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mOptimization failed with DeepLearningFramework.NUMPY interface of ModelCompiler.ONNX_RUNTIME. Got error [ONNXRuntimeError] : 6 : RUNTIME_EXCEPTION : Exception during initialization: /onnxruntime_src/onnxruntime/contrib_ops/cpu/quantization/qlinear_softmax.cc:74 onnxruntime::contrib::QLinearSoftmax::QLinearSoftmax(const onnxruntime::OpKernelInfo&) x_shape != nullptr && x_shape->dim_size() > 0 was false. input_shape of QLinearSoftmax must be existed\n",
            ". If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\u001b[0m\n",
            "\u001b[32m2023-02-11 12:53:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: None.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-11 12:53:22.069625382 [E:onnxruntime:, inference_session.cc:1499 operator()] Exception during initialization: /onnxruntime_src/onnxruntime/contrib_ops/cpu/quantization/qlinear_softmax.cc:74 onnxruntime::contrib::QLinearSoftmax::QLinearSoftmax(const onnxruntime::OpKernelInfo&) x_shape != nullptr && x_shape->dim_size() > 0 was false. input_shape of QLinearSoftmax must be existed\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
            "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
            "[ SUCCESS ] Generated IR version 11 model.\n",
            "[ SUCCESS ] XML file: /tmp/tmpg84zkk1x/fp32/temp.xml\n",
            "[ SUCCESS ] BIN file: /tmp/tmpg84zkk1x/fp32/temp.bin\n",
            "\u001b[32m2023-02-11 12:53:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0462033748626709 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 12:53:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: QuantizationType.HALF.\u001b[0m\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARNING ]  Use of deprecated cli option --data_type detected. Option use in the following releases will be fatal. \n",
            "Warning: One or more of the values of the Constant can't fit in the float16 data type. Those values were casted to the nearest limit value, the model can produce incorrect results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
            "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
            "[ SUCCESS ] Generated IR version 11 model.\n",
            "[ SUCCESS ] XML file: /tmp/tmpg84zkk1x/fp32/temp.xml\n",
            "[ SUCCESS ] BIN file: /tmp/tmpg84zkk1x/fp32/temp.bin\n",
            "\u001b[32m2023-02-11 12:53:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0430908203125 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 12:53:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with OpenVINOCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
            "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
            "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai/latest/openvino_2_0_transition_guide.html\n",
            "[ SUCCESS ] Generated IR version 11 model.\n",
            "[ SUCCESS ] XML file: /tmp/tmpg84zkk1x/fp32/temp.xml\n",
            "[ SUCCESS ] BIN file: /tmp/tmpg84zkk1x/fp32/temp.bin\n",
            "\u001b[32m2023-02-11 12:54:47\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mOptimization failed with DeepLearningFramework.NUMPY interface of ModelCompiler.OPENVINO. Got error could not append an elementwise post-op. If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\u001b[0m\n",
            "\n",
            "[Speedster results on 11th Gen Intel(R) Core(TM) i7-11800H @ 2.30GHz]\n",
            "┏━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
            "┃ Metric      ┃ Original Model   ┃ Optimized Model   ┃ Improvement   ┃\n",
            "┣━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━━╋━━━━━━━━━━━━━━━┫\n",
            "┃ backend     ┃ PYTORCH          ┃ OpenVINO          ┃               ┃\n",
            "┃ latency     ┃ 0.0523 sec/batch ┃ 0.0431 sec/batch  ┃ 1.21x         ┃\n",
            "┃ throughput  ┃ 19.12 data/sec   ┃ 23.21 data/sec    ┃ 1.21x         ┃\n",
            "┃ model size  ┃ 438.02 MB        ┃ 438.36 MB         ┃ 0%            ┃\n",
            "┃ metric drop ┃                  ┃ 0.0040            ┃               ┃\n",
            "┃ techniques  ┃                  ┃ int8_dynamic      ┃               ┃\n",
            "┗━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━━━━━┻━━━━━━━━━━━━━━━┛\n",
            "\n",
            "Max speed-up with your input parameters is 1.21x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
            "\n"
          ]
        }
      ],
      "source": [
        "optimized_model = optimize_model(\n",
        "    model=model,\n",
        "    input_data=encoded_inputs,\n",
        "    optimization_time=\"constrained\",\n",
        "    ignore_compilers=[\"tensor_rt\", \"tvm\"],  # TensorRT does not work for this model\n",
        "    dynamic_info=dynamic_info,\n",
        "    metric_drop_ths=0.1,\n",
        "    device='cpu',\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0fbfe6fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fbfe6fa",
        "outputId": "7b067f03-ec8a-4f70-c5b2-32d3385cfbab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average response time for original BERT: 56.99615478515625 ms\n"
          ]
        }
      ],
      "source": [
        "times = []\n",
        "\n",
        "# Warmup for 30 iterations\n",
        "for encoded_input in encoded_inputs[:30]:\n",
        "    with torch.no_grad():\n",
        "        final_out = model(**encoded_input)\n",
        "\n",
        "# Benchmark\n",
        "for encoded_input in encoded_inputs:\n",
        "    st = time.time()\n",
        "    with torch.no_grad():\n",
        "        final_out = model(**encoded_input)\n",
        "    times.append(time.time()-st)\n",
        "original_model_time = sum(times)/len(times)*1000\n",
        "print(f\"Average response time for original BERT: {original_model_time} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "10d17b5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10d17b5c",
        "outputId": "f26b4a17-b196-48de-85b2-705d9e75d851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average response time for optimized BERT (metric drop): 41.04656457901001 ms\n"
          ]
        }
      ],
      "source": [
        "times = []\n",
        "\n",
        "# Warmup for 30 iterations\n",
        "for encoded_input in encoded_inputs[:30]:\n",
        "    with torch.no_grad():\n",
        "        final_out = optimized_model(**encoded_input)\n",
        "\n",
        "# Benchmark\n",
        "for encoded_input in encoded_inputs:\n",
        "    st = time.time()\n",
        "    with torch.no_grad():\n",
        "        final_out = optimized_model(**encoded_input)\n",
        "    times.append(time.time()-st)\n",
        "optimized_model_time = sum(times)/len(times)*1000\n",
        "print(f\"Average response time for optimized BERT (metric drop): {optimized_model_time} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15711cf4",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "nebullvm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "9c5b325fcd5468045cb00d6f6e8552712249001c4af40fff338bf9bc94161db4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
