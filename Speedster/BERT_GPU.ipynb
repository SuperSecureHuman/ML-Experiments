{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7Ex-UkOgUHlV",
      "metadata": {
        "id": "7Ex-UkOgUHlV"
      },
      "source": [
        "## BERT GPU (Google Colab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d527d63b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d527d63b",
        "outputId": "9fdc3e35-8953-4f71-8578-421f5a397fd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_VISIBLE_DEVICES=0\n"
          ]
        }
      ],
      "source": [
        "%env CUDA_VISIBLE_DEVICES=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QFQh3BVr1-GO",
      "metadata": {
        "id": "QFQh3BVr1-GO"
      },
      "outputs": [],
      "source": [
        "!pip install speedster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cffbfa32",
      "metadata": {
        "id": "cffbfa32"
      },
      "outputs": [],
      "source": [
        "!python -m nebullvm.installers.auto_installer  --compilers all"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uwcUuw-aQNuF",
      "metadata": {
        "id": "uwcUuw-aQNuF"
      },
      "outputs": [],
      "source": [
        "!pip install pillow==9.0.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ioXMNXKR841",
      "metadata": {
        "id": "7ioXMNXKR841"
      },
      "outputs": [],
      "source": [
        "!pip install protobuf==3.19.6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73072506",
      "metadata": {
        "id": "73072506"
      },
      "source": [
        "## Model and Dataset setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4d55115",
      "metadata": {
        "id": "e4d55115"
      },
      "source": [
        "We chose BERT as the pre-trained model that we want to optimize. Let's download both the pre-trained model and the tokenizer from the Hugging Face model hub."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d633cf21",
      "metadata": {
        "id": "d633cf21",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased', torchscript=True)\n",
        "\n",
        "# Move the model to gpu if available and set eval mode\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11aa0739",
      "metadata": {
        "id": "11aa0739"
      },
      "source": [
        "Let's create an example dataset with some random sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "cbbfeeb2",
      "metadata": {
        "id": "cbbfeeb2"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "sentences = [\n",
        "    \"Mars is the fourth planet from the Sun.\",\n",
        "    \"has a crust primarily composed of elements\",\n",
        "    \"However, it is unknown\",\n",
        "    \"can be viewed from Earth\",\n",
        "    \"It was the Romans\",\n",
        "]\n",
        "\n",
        "len_dataset = 100\n",
        "\n",
        "texts = []\n",
        "for _ in range(len_dataset):\n",
        "    n_times = random.randint(1, 30)\n",
        "    texts.append(\" \".join(random.choice(sentences) for _ in range(n_times)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a09f9424",
      "metadata": {
        "id": "a09f9424"
      },
      "outputs": [],
      "source": [
        "encoded_inputs = [tokenizer(text, return_tensors=\"pt\") for text in texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17040431",
      "metadata": {
        "id": "17040431"
      },
      "source": [
        "## Speed up inference with Speedster: no metric drop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44ddc21d",
      "metadata": {
        "id": "44ddc21d"
      },
      "source": [
        "It's now time of improving a bit the performance in terms of speed. Let's use `Speedster`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f9d934f6",
      "metadata": {
        "id": "f9d934f6"
      },
      "outputs": [],
      "source": [
        "from speedster import optimize_model, save_model, load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76248033",
      "metadata": {
        "id": "76248033"
      },
      "source": [
        "Using Speedster is very simple and straightforward! Just use the `optimize_model` function and provide as input the model, some input data as example and the optimization time mode. Optionally a dynamic_info dictionary can be also provided, in order to support inputs with dynamic shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "zPC_EDwEJIM0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPC_EDwEJIM0",
        "outputId": "297c448a-6662-4f57-dc3f-5d7fdcfefd26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 07:02:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU\u001b[0m\n",
            "\u001b[32m2023-02-11 07:02:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
            "\u001b[32m2023-02-11 07:02:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.008934900760650635 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 07:02:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 07:03:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.01056361198425293 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 07:03:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 07:03:31\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
            "\u001b[32m2023-02-11 07:03:32\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mOptimization failed with DeepLearningFramework.PYTORCH interface of ModelCompiler.TORCHSCRIPT. Got error Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.HalfTensor instead (while checking arguments for embedding). If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\u001b[0m\n",
            "\u001b[32m2023-02-11 07:03:32\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 07:03:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.00930929183959961 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 07:03:37\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 07:03:51\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
            "\n",
            "[Speedster results on Tesla T4]\n",
            "Metric       Original Model    Optimized Model    Improvement\n",
            "-----------  ----------------  -----------------  -------------\n",
            "backend      PYTORCH           ONNXRuntime\n",
            "latency      0.0089 sec/batch  0.0093 sec/batch   0.96x\n",
            "throughput   111.92 data/sec   107.42 data/sec    0.96x\n",
            "model size   438.02 MB         438.23 MB          0%\n",
            "metric drop                    0\n",
            "techniques                     fp16\n",
            "\n",
            "Max speed-up with your input parameters is 0.96x. If you want to get a faster optimized model, see the following link for some suggestions: https://docs.nebuly.com/modules/speedster/getting-started/run-the-optimization#acceleration-suggestions\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dynamic_info = {\n",
        "    \"inputs\": [\n",
        "        {0: 'batch', 1: 'num_tokens'},\n",
        "        {0: 'batch', 1: 'num_tokens'},\n",
        "        {0: 'batch', 1: 'num_tokens'},\n",
        "    ],\n",
        "    \"outputs\": [\n",
        "        {0: 'batch', 1: 'num_tokens'},\n",
        "        {0: 'batch'},\n",
        "    ]\n",
        "}\n",
        "\n",
        "optimized_model = optimize_model(\n",
        "    model=model,\n",
        "    input_data=encoded_inputs,\n",
        "    optimization_time=\"constrained\",\n",
        "    ignore_compilers=[\"tensor_rt\", \"tvm\"],  # TensorRT does not work for this model\n",
        "    dynamic_info=dynamic_info,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "98c6ab09",
      "metadata": {
        "id": "98c6ab09"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "# Move inputs to gpu if available\n",
        "encoded_inputs = [tokenizer(text, return_tensors=\"pt\").to(device) for text in texts]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e5b3b21",
      "metadata": {
        "id": "6e5b3b21"
      },
      "source": [
        "Let's run the prediction 100 times to calculate the average response time of the original model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d3bc5c98",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3bc5c98",
        "outputId": "deeafd40-b903-43c6-def0-97283e1b5720"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average response time for original DistilBERT: 9.37291145324707 ms\n"
          ]
        }
      ],
      "source": [
        "times = []\n",
        "\n",
        "# Warmup for 30 iterations\n",
        "for encoded_input in encoded_inputs[:30]:\n",
        "    with torch.no_grad():\n",
        "        final_out = model(**encoded_input)\n",
        "\n",
        "# Benchmark\n",
        "for encoded_input in encoded_inputs:\n",
        "    st = time.time()\n",
        "    with torch.no_grad():\n",
        "        final_out = model(**encoded_input)\n",
        "    times.append(time.time()-st)\n",
        "original_model_time = sum(times)/len(times)*1000\n",
        "print(f\"Average response time for original DistilBERT: {original_model_time} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3db0a7a1",
      "metadata": {
        "id": "3db0a7a1"
      },
      "source": [
        "Let's run the prediction 100 times to calculate the average response time of the optimized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "a3e83997",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a3e83997",
        "outputId": "47765b66-b19d-4382-c25f-5532827f9c77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average response time for optimized BERT (no metric drop): 7.079629898071289 ms\n"
          ]
        }
      ],
      "source": [
        "times = []\n",
        "\n",
        "# Warmup for 30 iterations\n",
        "for encoded_input in encoded_inputs[:30]:\n",
        "    with torch.no_grad():\n",
        "        final_out = optimized_model(**encoded_input)\n",
        "\n",
        "# Benchmark\n",
        "for encoded_input in encoded_inputs:\n",
        "    st = time.time()\n",
        "    with torch.no_grad():\n",
        "        final_out = optimized_model(**encoded_input)\n",
        "    times.append(time.time()-st)\n",
        "optimized_model_time = sum(times)/len(times)*1000\n",
        "print(f\"Average response time for optimized BERT (no metric drop): {optimized_model_time} ms\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ceb60d8c",
      "metadata": {
        "id": "ceb60d8c"
      },
      "source": [
        "## Speed up inference with Speedster: metric drop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1950d5",
      "metadata": {
        "id": "7b1950d5"
      },
      "source": [
        "This time we will use the `metric_drop_ths` argument to accept a little drop in terms of precision, in order to enable quantization and obtain an higher speedup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "de5721d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de5721d8",
        "outputId": "17fc2c0f-9496-466b-b0fa-348fc8846507"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m2023-02-11 07:03:55\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mRunning Speedster on GPU\u001b[0m\n",
            "\u001b[32m2023-02-11 07:03:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mBenchmark performance of original model\u001b[0m\n",
            "\u001b[32m2023-02-11 07:04:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOriginal model latency: 0.012968626022338867 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 07:04:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 07:04:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.0058023929595947266 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 07:04:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with PytorchBackendCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 07:04:11\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mUnable to trace model with torch.fx\u001b[0m\n",
            "\u001b[32m2023-02-11 07:04:12\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mOptimization failed with DeepLearningFramework.PYTORCH interface of ModelCompiler.TORCHSCRIPT. Got error Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.HalfTensor instead (while checking arguments for embedding). If possible the compilation will be re-scheduled with another interface. Please consult the documentation for further info or open an issue on GitHub for receiving assistance.\u001b[0m\n",
            "\u001b[32m2023-02-11 07:04:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: None.\u001b[0m\n",
            "\u001b[32m2023-02-11 07:04:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.009480714797973633 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 07:04:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.HALF.\u001b[0m\n",
            "\u001b[32m2023-02-11 07:04:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimized model latency: 0.004265785217285156 sec/iter\u001b[0m\n",
            "\u001b[32m2023-02-11 07:04:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.DYNAMIC.\u001b[0m\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.0/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.0/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.1/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.1/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.2/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.2/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.3/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.3/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.4/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.4/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.5/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.5/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.6/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.6/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.7/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.7/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.8/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.8/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.9/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.9/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.10/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.10/attention/self/MatMul_1]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.11/attention/self/MatMul]\n",
            "Ignore MatMul due to non constant B: /[/core_model/encoder/layer.11/attention/self/MatMul_1]\n",
            "\u001b[32m2023-02-11 07:04:53\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
            "\u001b[32m2023-02-11 07:04:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1mOptimizing with ONNXCompiler and q_type: QuantizationType.STATIC.\u001b[0m\n",
            "\u001b[32m2023-02-11 07:05:40\u001b[0m | \u001b[38;2;211;211;211mWARNING \u001b[0m | \u001b[38;2;211;211;211mThe optimized model will be discarded due to poor results obtained with the given metric.\u001b[0m\n",
            "\n",
            "[Speedster results on Tesla T4]\n",
            "Metric       Original Model    Optimized Model    Improvement\n",
            "-----------  ----------------  -----------------  -------------\n",
            "backend      PYTORCH           ONNXRuntime\n",
            "latency      0.0130 sec/batch  0.0043 sec/batch   3.04x\n",
            "throughput   77.11 data/sec    234.42 data/sec    3.04x\n",
            "model size   438.03 MB         219.36 MB          -49%\n",
            "metric drop                    0.0083\n",
            "techniques                     fp16\n"
          ]
        }
      ],
      "source": [
        "optimized_model = optimize_model(\n",
        "    model=model,\n",
        "    input_data=encoded_inputs,\n",
        "    optimization_time=\"constrained\",\n",
        "    ignore_compilers=[\"tensor_rt\", \"tvm\"],  # TensorRT does not work for this model\n",
        "    dynamic_info=dynamic_info,\n",
        "    metric_drop_ths=0.1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "0fbfe6fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fbfe6fa",
        "outputId": "7b067f03-ec8a-4f70-c5b2-32d3385cfbab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average response time for original BERT: 9.293725490570068 ms\n"
          ]
        }
      ],
      "source": [
        "times = []\n",
        "\n",
        "# Warmup for 30 iterations\n",
        "for encoded_input in encoded_inputs[:30]:\n",
        "    with torch.no_grad():\n",
        "        final_out = model(**encoded_input)\n",
        "\n",
        "# Benchmark\n",
        "for encoded_input in encoded_inputs:\n",
        "    st = time.time()\n",
        "    with torch.no_grad():\n",
        "        final_out = model(**encoded_input)\n",
        "    times.append(time.time()-st)\n",
        "original_model_time = sum(times)/len(times)*1000\n",
        "print(f\"Average response time for original BERT: {original_model_time} ms\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "10d17b5c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10d17b5c",
        "outputId": "f26b4a17-b196-48de-85b2-705d9e75d851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average response time for optimized BERT (metric drop): 3.913660049438477 ms\n"
          ]
        }
      ],
      "source": [
        "times = []\n",
        "\n",
        "# Warmup for 30 iterations\n",
        "for encoded_input in encoded_inputs[:30]:\n",
        "    with torch.no_grad():\n",
        "        final_out = optimized_model(**encoded_input)\n",
        "\n",
        "# Benchmark\n",
        "for encoded_input in encoded_inputs:\n",
        "    st = time.time()\n",
        "    with torch.no_grad():\n",
        "        final_out = optimized_model(**encoded_input)\n",
        "    times.append(time.time()-st)\n",
        "optimized_model_time = sum(times)/len(times)*1000\n",
        "print(f\"Average response time for optimized BERT (metric drop): {optimized_model_time} ms\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "f401cf1dbab24df559ae8789ef7eacae25a0fecff741eceb08aecb7249ab0875"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
